{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# Build a RAG-based, GenAI chatbot using Structured Data with Snowflake and Fivetran\n",
    "\n",
    "This notebook includes the scripts to complement this [QuickStart Guide](https://quickstarts.snowflake.com/guide/fivetran_vineyard_assistant_chatbot/#0). Please view this quickstart and follow for the step by step directions. This notebook includes the additional setup scripts that you will see in this guide, including:\n",
    "- Script to create the transformations, in this case to concatenate various strings for each winery\n",
    "- Script to create embeddings\n",
    "- Python/Streamlit code to create the Streamlit Application\n",
    "\n",
    "You can import this notebook directly into Snowflake for an optimal notebook experience!\n",
    "\n",
    "Note you will need to make sure you first set the appropriate context. For example:\n",
    "```\n",
    "USE ROLE <ROLE>;\n",
    "USE DATABASE <DATABASE>;\n",
    "USE SCHEMA <SCHEMA>;\n",
    "```\n",
    "\n",
    "You will also need to make sure the role you are using has the ability to use Cortex. See the documentation [here](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#required-privileges).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "/** Create each winery and vineyard review as a single field vs multiple fields **/\n",
    "    CREATE or REPLACE TABLE vineyard_data_single_string AS \n",
    "        SELECT WINERY_OR_VINEYARD, CONCAT(' The winery name is ', IFNULL(WINERY_OR_VINEYARD, ' Name is not known')\n",
    "        , ' and resides in the California wine region of ', IFNULL(CA_WINE_REGION, 'unknown'), '.'\n",
    "        , ' The AVA Appellation is the ', IFNULL(AVA_APPELLATION_SUB_APPELLATION, 'unknown'), '.'\n",
    "        , ' The website associated with the winery is ', IFNULL(WEBSITE, 'unknown'), '.'\n",
    "        , ' The price range is ', IFNULL(PRICE_RANGE, 'unknown'), '.'\n",
    "        , ' Tasting Room Hours: ', IFNULL(TASTING_ROOM_HOURS, 'unknown'), '.'\n",
    "        , ' The reservation requirement is: ', IFNULL(RESERVATION_REQUIRED, 'unknown'), '.'\n",
    "        , ' The Winery Description is: ', IFNULL(WINERY_DESCRIPTION, 'unknown'), ''\n",
    "        , ' The Primary Varietals this winery offers is ', IFNULL(PRIMARY_VARIETALS, 'unknown'), '.'\n",
    "        , ' Thoughts on the Tasting Room Experience: ', IFNULL(TASTING_ROOM_EXPERIENCE, 'unknown'), '.'\n",
    "        , ' Amenities: ', IFNULL(AMENITIES, 'unknown'), '.'\n",
    "        , ' Awards and Accolades: ', IFNULL(AWARDS_AND_ACCOLADES, 'unknown'), '.'\n",
    "        , ' Distance Travel Time considerations: ', IFNULL(DISTANCE_AND_TRAVEL_TIME, 'unknown'), '.'\n",
    "        , ' User Rating: ', IFNULL(USER_RATING, 'unknown'), '.'\n",
    "        , ' The Secondary Varietals for this winery: ', IFNULL(SECONDARY_VARIETALS, 'unknown'), '.'\n",
    "        , ' Wine Styles: ', IFNULL(WINE_STYLES, 'unknown'), '.'\n",
    "        , ' Events and Activities: ', IFNULL(EVENTS_AND_ACTIVITIES, 'unknown'), '.'\n",
    "        , ' Sustainability Practices: ', IFNULL(SUSTAINABILITY_PRACTICES, 'unknown'), '.'\n",
    "        , ' Social Media Channels: ', IFNULL(SOCIAL_MEDIA, 'unknown'), ''\n",
    "        , ' The address is ', IFNULL(ADDRESS, 'unknown'), ', '\n",
    "        , IFNULL(CITY, 'unknown'), ', '\n",
    "        , IFNULL(STATE, 'unknown'), ', '\n",
    "        , IFNULL(cast(ZIP as varchar(10)), 'unknown'), '.'\n",
    "        , ' The Phone Number is ', IFNULL(PHONE, 'unknown'), '.'\n",
    "        , ' The Winemaker is ', IFNULL(WINEMAKER, 'unknown'), '.'\n",
    "        , ' Did Kelly Kohlleffel recommend this winery?: ', IFNULL(KELLY_KOHLLEFFEL_RECOMMENDED, 'unknown'), ''\n",
    "    ) AS winery_information FROM california_wine_country_visits;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "Now create table that includes the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced0e35-9339-4465-8b0a-babd7c18204f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "    /** Create the vector table from the wine review single field table **/\n",
    "      CREATE or REPLACE TABLE vineyard_data_vectors AS \n",
    "            SELECT winery_or_vineyard, winery_information, \n",
    "            snowflake.cortex.EMBED_TEXT_768('e5-base-v2', winery_information) as WINERY_EMBEDDING \n",
    "            FROM vineyard_data_single_string;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996167a-e6ba-4ea5-b12c-c7145a641968",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "Follow the quickstart instructions [here](https://quickstarts.snowflake.com/guide/fivetran_vineyard_assistant_chatbot/#5) and copy and paste this code into for your Streamlit application.\n",
    "\n",
    "```python\n",
    "#\n",
    "# Fivetran Snowflake Cortex Lab\n",
    "# Build a California Wine Assistant Chatbot\n",
    "#\n",
    "\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Change this list as needed to add/remove model capabilities.\n",
    "MODELS = [\n",
    "    \"gemma-7b\",\n",
    "    \"mistral-7b\",\n",
    "    \"mixtral-8x7b\",\n",
    "    \"reka-flash\",\n",
    "    \"jamba-instruct\", \n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3.1-70b\",\n",
    "    \"mistral-large2\",\n",
    "    \"llama3.1-405b\"\n",
    "]\n",
    "\n",
    "# Change this value to control the number of tokens you allow the user to change to control RAG context. In\n",
    "# this context for the data used, 1 chunk would be approximately 200-400 tokens.  So a limit is placed here\n",
    "# so that the LLM does not abort if the context is too large.\n",
    "CHUNK_NUMBER = [4,6,8,10,12,14,16]\n",
    "\n",
    "def build_layout():\n",
    "    #\n",
    "    # Builds the layout for the app side and main panels and return the question from the dynamic text_input control.\n",
    "    #\n",
    "\n",
    "    # Setup the state variables.\n",
    "    # Resets text input ID to enable it to be cleared since currently there is no native clear.\n",
    "    if 'reset_key' not in st.session_state: \n",
    "        st.session_state.reset_key = 0\n",
    "    # Holds the list of responses so the user can see changes while selecting other models and settings.\n",
    "    if 'conversation_state' not in st.session_state:\n",
    "        st.session_state.conversation_state = []\n",
    "\n",
    "    # Build the layout.\n",
    "    #\n",
    "    # Note:  Do not alter the manner in which the objects are laid out.  Streamlit requires this order because of references.\n",
    "    #\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "    st.title(\":wine_glass: California Wine Country Visit Assistant :wine_glass:\")\n",
    "    st.write(\"\"\"I'm an interactive California Wine Country Visit Assistant. A bit about me...I'm a RAG-based, Gen AI app **built \n",
    "      with and powered by Fivetran, Snowflake, Streamlit, and Cortex** and I use a custom, structured dataset!\"\"\")\n",
    "    st.caption(\"\"\"Let me help plan your trip to California wine country. Using the dataset you just moved into the Snowflake Data \n",
    "      Cloud with Fivetran, I'll assist you with winery and vineyard information and provide visit recommendations from numerous \n",
    "      models available in Snowflake Cortex (including Snowflake Arctic). You can even pick the model you want to use or try out \n",
    "      all the models. The dataset includes over **700 wineries and vineyards** across all CA wine-producing regions including the \n",
    "      North Coast, Central Coast, Central Valley, South Coast and various AVAs sub-AVAs. Let's get started!\"\"\")\n",
    "    user_question_placeholder = \"Message your personal CA Wine Country Visit Assistant...\"\n",
    "    st.sidebar.selectbox(\"Select a Snowflake Cortex model:\", MODELS, key=\"model_name\", index=3)\n",
    "    st.sidebar.checkbox('Use your Fivetran dataset as context?', key=\"dataset_context\", help=\"\"\"This turns on RAG where the \n",
    "    data replicated by Fivetran and curated in Snowflake will be used to add to the context of the LLM prompt.\"\"\")\n",
    "    if st.button('Reset conversation', key='reset_conversation_button'):\n",
    "        st.session_state.conversation_state = []\n",
    "        st.session_state.reset_key += 1\n",
    "        st.experimental_rerun()\n",
    "    processing_placeholder = st.empty()\n",
    "    question = st.text_input(\"\", placeholder=user_question_placeholder, key=f\"text_input_{st.session_state.reset_key}\", \n",
    "                             label_visibility=\"collapsed\")\n",
    "    if st.session_state.dataset_context:\n",
    "        st.caption(\"\"\"Please note that :green[**_I am_**] using your Fivetran dataset as context. All models are very \n",
    "          creative and can make mistakes. Consider checking important information before heading out to wine country.\"\"\")\n",
    "    else:\n",
    "        st.caption(\"\"\"Please note that :red[**_I am NOT_**] using your Fivetran dataset as context. All models are very \n",
    "          creative and can make mistakes. Consider checking important information before heading out to wine country.\"\"\")\n",
    "    with st.sidebar.expander(\"Advanced Options\"):\n",
    "        st.selectbox(\"Select number of context chunks:\", CHUNK_NUMBER, key=\"num_retrieved_chunks\", help=\"\"\"Adjust based on the \n",
    "        expected number of records/chunks of your data to be sent with the prompt before Cortext calls the LLM.\"\"\", index=1)\n",
    "    st.sidebar.caption(\"\"\"I use **Snowflake Cortex** which provides instant access to industry-leading large language models (LLMs), \n",
    "      including **Snowflake Arctic**, trained by researchers at companies like Mistral, Meta, Google, Reka, and Snowflake.\\n\\nCortex \n",
    "      also offers models that Snowflake has fine-tuned for specific use cases. Since these LLMs are fully hosted and managed by \n",
    "      Snowflake, using them requires no setup. My data stays within Snowflake, giving me the performance, scalability, and governance \n",
    "      you expect.\"\"\")\n",
    "    for _ in range(6):\n",
    "        st.sidebar.write(\"\")\n",
    "    url = 'https://i.imgur.com/9lS8Y34.png'\n",
    "    col1, col2, col3 = st.sidebar.columns([1,2,1])\n",
    "    with col2:\n",
    "        st.image(url, width=150)\n",
    "    caption_col1, caption_col2, caption_col3 = st.sidebar.columns([0.22,2,0.005])\n",
    "    with caption_col2:\n",
    "        st.caption(\"Fivetran, Snowflake, Streamlit, & Cortex\")\n",
    "\n",
    "    return question\n",
    "\n",
    "def build_prompt (question):\n",
    "    #\n",
    "    # Format the prompt based on if the user chooses to use the RAG option or not.\n",
    "    #\n",
    "\n",
    "    # Build the RAG prompt if the user chooses.  Defaulting the similarity to 0 -> 1 for better matching.\n",
    "    chunks_used = []\n",
    "    if st.session_state.dataset_context:\n",
    "        # Get the RAG records.\n",
    "        context_cmd = f\"\"\"\n",
    "          with context_cte as\n",
    "          (select winery_or_vineyard, winery_information as winery_chunk, vector_cosine_similarity(winery_embedding,\n",
    "                snowflake.cortex.embed_text_768('e5-base-v2', ?)) as v_sim\n",
    "          from vineyard_data_vectors\n",
    "          having v_sim > 0\n",
    "          order by v_sim desc\n",
    "          limit ?)\n",
    "          select winery_or_vineyard, winery_chunk from context_cte \n",
    "          \"\"\"\n",
    "        chunk_limit = st.session_state.num_retrieved_chunks\n",
    "        context_df = session.sql(context_cmd, params=[question, chunk_limit]).to_pandas()\n",
    "        context_len = len(context_df) -1\n",
    "        # Add the vineyard names to a list to be displayed later.\n",
    "        chunks_used = context_df['WINERY_OR_VINEYARD'].tolist()\n",
    "        # Build the additional prompt context using the wine dataset.\n",
    "        rag_context = \"\"\n",
    "        for i in range (0, context_len):\n",
    "            rag_context += context_df.loc[i, 'WINERY_CHUNK']\n",
    "        rag_context = rag_context.replace(\"'\", \"''\")\n",
    "        # Construct the prompt.\n",
    "        new_prompt = f\"\"\"\n",
    "          Act as a California winery visit expert for visitors to California wine country who want an incredible visit and \n",
    "          tasting experience. You are a personal visit assistant named Snowflake CA Wine Country Visit Assistant. Provide \n",
    "          the most accurate information on California wineries based only on the context provided. Only provide information \n",
    "          if there is an exact match below.  Do not go outside the context provided.  \n",
    "          Context: {rag_context}\n",
    "          Question: {question} \n",
    "          Answer: \n",
    "          \"\"\"\n",
    "    else:\n",
    "        # Construct the generic version of the prompt without RAG to only go against what the LLM was trained.\n",
    "        new_prompt = f\"\"\"\n",
    "          Act as a California winery visit expert for visitors to California wine country who want an incredible visit and \n",
    "          tasting experience. You are a personal visit assistant named Snowflake CA Wine Country Visit Assistant. Provide \n",
    "          the most accurate information on California wineries.\n",
    "          Question: {question} \n",
    "          Answer: \n",
    "          \"\"\"\n",
    "\n",
    "    return new_prompt, chunks_used\n",
    "\n",
    "def get_model_token_count(prompt_or_response) -> int:\n",
    "    #\n",
    "    # Calculate and return the token count for the model and prompt or response.\n",
    "    #\n",
    "    token_count = 0\n",
    "    try:\n",
    "        token_cmd = f\"\"\"select SNOWFLAKE.CORTEX.COUNT_TOKENS(?, ?) as token_count;\"\"\"\n",
    "        tc_data = session.sql(token_cmd, params=[st.session_state.model_name, prompt_or_response]).collect()\n",
    "        token_count = tc_data[0][0]\n",
    "    except Exception:\n",
    "        # Negative value just denoting that tokens could not be counted for some reason.\n",
    "        token_count = -9999\n",
    "\n",
    "    return token_count\n",
    "\n",
    "def calc_times(start_time, first_token_time, end_time, token_count):\n",
    "    #\n",
    "    # Calculate the times for the execution steps.\n",
    "    #\n",
    "\n",
    "    # Calculate the correct durations\n",
    "    time_to_first_token = first_token_time - start_time  # Time to the first token\n",
    "    total_duration = end_time - start_time  # Total time to generate all tokens\n",
    "    time_for_remaining_tokens = total_duration - time_to_first_token  # Time for the remaining tokens\n",
    "    \n",
    "    # Calculate tokens per second rate\n",
    "    tokens_per_second = token_count / total_duration if total_duration > 0 else 1\n",
    "    \n",
    "    # Ensure that time to first token is realistically non-zero\n",
    "    if time_to_first_token < 0.01:  # Adjust the threshold as needed\n",
    "        time_to_first_token = total_duration / 2  # A rough estimate if it's too small\n",
    "\n",
    "    return time_to_first_token, time_for_remaining_tokens, tokens_per_second\n",
    "\n",
    "def run_prompt(question):\n",
    "    #\n",
    "    # Run the prompt against Cortex.\n",
    "    #\n",
    "    formatted_prompt, chunks_used = build_prompt (question)\n",
    "    token_count = get_model_token_count(formatted_prompt)\n",
    "    start_time = time.time()\n",
    "    cortex_cmd = f\"\"\"\n",
    "             select SNOWFLAKE.CORTEX.COMPLETE(?,?) as response\n",
    "           \"\"\"    \n",
    "    sql_resp = session.sql(cortex_cmd, params=[st.session_state.model_name, formatted_prompt])\n",
    "    first_token_time = time.time() \n",
    "    answer_df = sql_resp.collect()\n",
    "    end_time = time.time()\n",
    "    time_to_first_token, time_for_remaining_tokens, tokens_per_second = calc_times(start_time, first_token_time, end_time, token_count)\n",
    "\n",
    "    return answer_df, time_to_first_token, time_for_remaining_tokens, tokens_per_second, int(token_count), chunks_used\n",
    "\n",
    "def main():\n",
    "    #\n",
    "    # Controls the flow of the app.\n",
    "    #\n",
    "    question = build_layout()\n",
    "    if question:\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            try:\n",
    "                # Run the prompt.\n",
    "                token_count = 0\n",
    "                data, time_to_first_token, time_for_remaining_tokens, tokens_per_second, token_count, chunks_used = run_prompt(question)\n",
    "                response = data[0][0]\n",
    "                # Add the response token count to the token total so we get a better prediction of the costs.\n",
    "                if response:\n",
    "                    token_count += get_model_token_count(response)\n",
    "                    # Conditionally append the token count line based on the checkbox\n",
    "                    rag_delim = \", \"\n",
    "                    st.session_state.conversation_state.append(\n",
    "                        (f\":information_source: RAG Chunks/Records Used:\",\n",
    "                         f\"\"\"<span style='color:#808080;'> {(rag_delim.join([str(ele) for ele in chunks_used])) if chunks_used else 'none'} \n",
    "                         </span><br/><br/>\"\"\")\n",
    "                    )\n",
    "                    st.session_state.conversation_state.append(\n",
    "                        (f\":1234: Token Count for '{st.session_state.model_name}':\", \n",
    "                         f\"\"\"<span style='color:#808080;'>{token_count} tokens • {tokens_per_second:.2f} tokens/s • \n",
    "                         {time_to_first_token:.2f}s to first token + {time_for_remaining_tokens:.2f}s.</span>\"\"\")\n",
    "                    )\n",
    "                    # Append the new results.\n",
    "                    st.session_state.conversation_state.append((f\"CA Wine Country Visit Assistant ({st.session_state.model_name}):\", response))\n",
    "                    st.session_state.conversation_state.append((\"You:\", question))\n",
    "            except Exception as e:\n",
    "                st.warning(f\"An error occurred while processing your question: {e}\")\n",
    "        \n",
    "        # Display the results in a stacked format.\n",
    "        if st.session_state.conversation_state:\n",
    "            for i in reversed(range(len(st.session_state.conversation_state))):\n",
    "                label, message = st.session_state.conversation_state[i]\n",
    "                if 'Token Count' in label or 'RAG Chunks' in label:\n",
    "                    # Display the token count in a specific format\n",
    "                    st.markdown(f\"**{label}** {message}\", unsafe_allow_html=True)\n",
    "                elif i % 2 == 0:\n",
    "                    st.write(f\":wine_glass:**{label}** {message}\")\n",
    "                else:\n",
    "                    st.write(f\":question:**{label}** {message}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #\n",
    "    # App startup method.\n",
    "    #\n",
    "    session = get_active_session()\n",
    "    \n",
    "    main()\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
